"""
Ğ¡ĞĞ”Ğ•Ğ Ğ–ĞĞĞ˜Ğ•:
1. ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ (NLTK, Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹)
2. NLP + TF-IDF (3 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ)
3. Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯
4. ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ĞœĞĞ”Ğ•Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ•
5. PANDAS Ğ¨ĞŸĞĞ Ğ“ĞĞ›ĞšĞ (Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹)
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import pandas as pd
import numpy as np
import re
import string
from scipy.optimize import minimize, curve_fit
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

# â•â•â• Ğ¡ĞšĞĞ§ĞĞ¢Ğ¬ NLTK Ğ”ĞĞĞĞ«Ğ• (ĞĞ”Ğ˜Ğ Ğ ĞĞ—) â•â•â•
# nltk.download('stopwords')
# nltk.download('punkt')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. NLP + TF-IDF
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â• ĞŸĞ Ğ•Ğ”ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ â•â•â•
def preprocess_text(text, language='russian', 
                    remove_numbers=True, 
                    remove_stopwords=True, 
                    stem=True):
    """
    ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ°
    
    ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:
    - language: 'russian' Ğ¸Ğ»Ğ¸ 'english'
    - remove_numbers: ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ Ñ‡Ğ¸ÑĞ»Ğ°?
    - remove_stopwords: ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ°?
    - stem: Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑÑ‚ĞµĞ¼Ğ¼Ğ¸Ğ½Ğ³?
    """
    stop_words = set(stopwords.words(language))
    stemmer = SnowballStemmer(language)
    
    # 1. ĞĞ¸Ğ¶Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€
    text = text.lower()
    
    # 2. Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ñ‡Ğ¸ÑĞ»Ğ°
    if remove_numbers:
        text = re.sub(r'\d+', '', text)
    
    # 3. Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ñ
    text = ''.join([c for c in text if c not in string.punctuation])
    
    # 4. Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ)
    words = text.split()
    
    # 5. Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ°
    if remove_stopwords:
        words = [w for w in words if w not in stop_words]
    
    # 6. Ğ¡Ñ‚ĞµĞ¼Ğ¼Ğ¸Ğ½Ğ³
    if stem:
        words = [stemmer.stem(w) for w in words]
    
    return " ".join(words)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 1: ĞšĞ›ĞĞ¡Ğ¡Ğ˜Ğ¤Ğ˜ĞšĞĞ¦Ğ˜Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢ĞĞ’ (Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ vs Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def nlp_classification(csv_file, text_column, label_column=None):
    """
    ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° 2 ĞºĞ»Ğ°ÑÑĞ°
    
    ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:
    - csv_file: Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ
    - text_column: Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
    - label_column: Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ñ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² ĞºĞ»Ğ°ÑÑĞ° 1
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°
    df = pd.read_csv(csv_file)
    
    # ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
    df['text_clean'] = df[text_column].apply(preprocess_text)
    
    # Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    vectorizer = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),  # ÑƒĞ½Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ + Ğ±Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹
        min_df=2,
        max_df=0.8
    )
    """
    # ============ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ£ĞĞ˜Ğ“Ğ ĞĞœĞœĞ« ============
    vectorizer = TfidfVectorizer(ngram_range=(1, 1))
    # Ğ¡Ğ»Ğ¾Ğ²Ğ°: ['Ğ¾Ñ‚ĞµĞ»ÑŒ', 'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹', 'Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»']

    # ============ Ğ£ĞĞ˜Ğ“Ğ ĞĞœĞœĞ« + Ğ‘Ğ˜Ğ“Ğ ĞĞœĞœĞ« ============
    vectorizer = TfidfVectorizer(ngram_range=(1, 2))
    # Ğ¡Ğ»Ğ¾Ğ²Ğ°: ['Ğ¾Ñ‚ĞµĞ»ÑŒ', 'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹', 'Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»', 'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞµĞ»ÑŒ', 'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»']

    # ============ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ‘Ğ˜Ğ“Ğ ĞĞœĞœĞ« ============
    vectorizer = TfidfVectorizer(ngram_range=(2, 2))
    # Ğ¡Ğ»Ğ¾Ğ²Ğ°: ['Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞµĞ»ÑŒ', 'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»']

    # ============ Ğ£ĞĞ˜Ğ“Ğ ĞĞœĞœĞ« + Ğ‘Ğ˜Ğ“Ğ ĞĞœĞœĞ« + Ğ¢Ğ Ğ˜Ğ“Ğ ĞĞœĞœĞ« ============
    vectorizer = TfidfVectorizer(ngram_range=(1, 3))
    # ĞœĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²! ĞœĞ¾Ğ¶ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ
    """
    X = vectorizer.fit_transform(df['text_clean'])
    
    # Ğ•ÑĞ»Ğ¸ Ğ¼ĞµÑ‚ĞºĞ¸ ĞµÑÑ‚ÑŒ - Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼
    if label_column:
        y = df[label_column]
        
        # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )
        
        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        model = LogisticRegression(max_iter=1000, class_weight='balanced')
        model.fit(X_train, y_train)
        
        # ĞÑ†ĞµĞ½ĞºĞ°
        predictions = model.predict(X_test)
        print(f"Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: {accuracy_score(y_test, predictions):.3f}")
        
        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ’Ğ¡Ğ•Ğ¥ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        all_predictions = model.predict(X)
        answer = (all_predictions == 1).sum()
        
        # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°
        feature_names = vectorizer.get_feature_names_out()
        coefficients = model.coef_[0]
        
        top_class1 = pd.DataFrame({
            'word': feature_names,
            'coef': coefficients
        }).sort_values('coef', ascending=False).head(10)
        
        print("\nĞ¢Ğ¾Ğ¿-10 ÑĞ»Ğ¾Ğ² ĞºĞ»Ğ°ÑÑĞ° 1:")
        print(top_class1)
        
        return answer
    
    # Ğ•ÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ½ĞµÑ‚ - Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹
    return X, vectorizer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 2: ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢ĞĞ’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def nlp_clustering(csv_file, text_column, n_clusters=2):
    """
    ĞšĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²
    
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚: Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ², ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°
    df = pd.read_csv(csv_file)
    df['text_clean'] = df[text_column].apply(preprocess_text)
    
    # Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X = vectorizer.fit_transform(df['text_clean'])
    
    # ĞšĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X)
    
    df['cluster'] = clusters
    
    # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ²
    feature_names = vectorizer.get_feature_names_out()
    
    for i in range(n_clusters):
        print(f"\n{'='*50}")
        print(f"ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ  {i} (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€: {(clusters == i).sum()})")
        print(f"{'='*50}")
        
        # Ğ¢Ğ¾Ğ¿-15 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
        cluster_center = kmeans.cluster_centers_[i]
        top_indices = cluster_center.argsort()[-15:][::-1]
        top_words = [feature_names[idx] for idx in top_indices]
        
        print(f"ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°: {', '.join(top_words)}")
    
    # ĞĞ¢Ğ’Ğ•Ğ¢: Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, ĞºĞ°ĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€ - Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹
    # ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ĞµÑĞ»Ğ¸ Ğ² ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğµ 0 ÑĞ»Ğ¾Ğ²Ğ° "Ğ¾Ñ‚ĞµĞ»ÑŒ", "Ğ½Ğ¾Ğ¼ĞµÑ€" â†’ ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹
    review_cluster = 0  # Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ¬ Ğ’Ğ Ğ£Ğ§ĞĞ£Ğ®!
    
    answer = (clusters == review_cluster).sum()
    print(f"\nğŸ¯ ĞĞ¢Ğ’Ğ•Ğ¢: {answer}")
    
    return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 3: ĞŸĞĞ˜Ğ¡Ğš ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ¥ Ğ¡Ğ›ĞĞ’ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def nlp_keywords(csv_file, text_column, doc_id=0, top_n=10):
    """
    Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
    
    ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:
    - doc_id: Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° (Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ¼ĞµÑ€ ÑÑ‚Ñ€Ğ¾ĞºĞ¸)
    - top_n: ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ
    """
    df = pd.read_csv(csv_file)
    df['text_clean'] = df[text_column].apply(preprocess_text)
    
    # Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(df['text_clean'])
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ TF-IDF Ğ´Ğ»Ñ Ğ½ÑƒĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
    doc_vector = X[doc_id].toarray().flatten()
    feature_names = vectorizer.get_feature_names_out()
    
    # Ğ¢Ğ¾Ğ¿-N ÑĞ»Ğ¾Ğ²
    keywords_df = pd.DataFrame({
        'word': feature_names,
        'tfidf': doc_vector
    }).sort_values('tfidf', ascending=False)
    
    top_keywords = keywords_df[keywords_df['tfidf'] > 0].head(top_n)
    print(top_keywords)
    
    # ĞĞ¢Ğ’Ğ•Ğ¢ (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹ ÑĞ»Ğ¾Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ)
    answer = ', '.join(top_keywords['word'].tolist())
    return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 4: ĞŸĞĞ˜Ğ¡Ğš ĞŸĞĞ¥ĞĞ–Ğ˜Ğ¥ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢ĞĞ’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def nlp_similar_docs(csv_file, text_column, target_id, top_n=5):
    """
    ĞĞ°Ğ¹Ñ‚Ğ¸ N ÑĞ°Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    """
    df = pd.read_csv(csv_file)
    df['text_clean'] = df[text_column].apply(preprocess_text)
    
    # Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    vectorizer = TfidfVectorizer(max_features=3000)
    X = vectorizer.fit_transform(df['text_clean'])
    
    # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾
    target_vector = X[target_id]
    similarities = cosine_similarity(target_vector, X).flatten()
    
    # Ğ¢Ğ¾Ğ¿-N (Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ°Ğ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚)
    top_indices = similarities.argsort()[-top_n-1:][::-1]
    top_indices = [idx for idx in top_indices if idx != target_id][:top_n]
    
    # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° 'id'
    if 'id' in df.columns:
        similar_ids = df.iloc[top_indices]['id'].tolist()
        answer = ', '.join(map(str, similar_ids))
    else:
        answer = ', '.join(map(str, top_indices))
    
    print(f"ĞŸĞ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: {answer}")
    return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def logistic_regression_template(csv_file, target_column):
    """
    Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
    
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ»Ğ°ÑÑĞ° 1
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°
    df = pd.read_csv(csv_file)
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ
    X = df.drop(target_column, axis=1)
    y = df[target_column]
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ²
    print("Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¾Ğ²:")
    print(y.value_counts(normalize=True))
    
    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    # X = pd.get_dummies(X, drop_first=True)
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    
    # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
    model = LogisticRegression(
        max_iter=10000,
        class_weight='balanced',  # Ğ”Ğ»Ñ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        random_state=42
    )
    model.fit(X_train_scaled, y_train)
    
    # ĞÑ†ĞµĞ½ĞºĞ°
    predictions = model.predict(X_test_scaled)
    print(f"\nĞ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: {accuracy_score(y_test, predictions):.3f}")
    print(classification_report(y_test, predictions))
    
    # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ’Ğ¡Ğ•Ğ¥ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    all_data_scaled = scaler.transform(X)
    all_predictions = model.predict(all_data_scaled)
    
    answer = (all_predictions == 1).sum()
    print(f"\nğŸ¯ ĞĞ¢Ğ’Ğ•Ğ¢: {answer}")
    
    return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ĞœĞĞ”Ğ•Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â• ĞœĞ•Ğ¢ĞĞ” 1: MINIMIZE (Ğ£ĞĞ˜Ğ’Ğ•Ğ Ğ¡ĞĞ›Ğ¬ĞĞ«Ğ™) â•â•â•
def fit_with_minimize(x, y, model_func, init_guess):
    """
    ĞŸĞ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
    
    ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:
    - x, y: Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    - model_func: Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (x, *params)
    - init_guess: Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ [a, b, ...]
    """
    def error_func(params):
        predictions = model_func(x, *params)
        return np.sum((y - predictions)**2)
    
    result = minimize(error_func, x0=init_guess, method='BFGS')
    
    return result.x, result.fun


# â•â•â• ĞœĞ•Ğ¢ĞĞ” 2: CURVE_FIT (Ğ”Ğ›Ğ¯ Ğ¯Ğ’ĞĞ«Ğ¥ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ™) â•â•â•
def fit_with_curve_fit(x, y, model_func, init_guess):
    """
    Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ ÑĞ²Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ y = f(x)
    """
    params, _ = curve_fit(model_func, x, y, p0=init_guess, maxfev=10000)
    
    predictions = model_func(x, *params)
    error = np.sum((y - predictions)**2)
    
    return params, error


# â•â•â• ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ â•â•â•

# Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ: y = a*x + b
def linear_model(x, a, b):
    return a*x + b

# Ğ¡Ğ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ğ°: y = sin(a*x) + b
def sin_model(x, a, b):
    return np.sin(a*x) + b

# ĞŸĞ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ + ÑĞ¸Ğ½ÑƒÑ (Ğ¤Ğ˜ĞĞĞ› 2024-25!)
def forecast_model(t, a1, a2, a3, a4):
    """model(t) = a1 + a2*t + a3*tÂ² + a4*sinÂ²(2Ï€*t/13)"""
    return a1 + a2*t + a3*t**2 + a4*np.sin(2*np.pi*t/13)

# Ğ­Ğ»Ğ»Ğ¸Ğ¿Ñ (ĞĞ•Ğ¯Ğ’ĞĞĞ• Ğ£Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ•! - Ğ¤Ğ˜ĞĞĞ› 2023-24)
def fit_ellipse(x, y):
    """xÂ²/aÂ² + yÂ²/bÂ² = 1"""
    def ellipse_error(params):
        a, b = params
        residuals = (x**2 / a**2) + (y**2 / b**2) - 1
        return np.sum(residuals**2)
    
    # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ
    init_guess = [np.abs(x).max() * 1.2, np.abs(y).max() * 1.2]
    
    result = minimize(ellipse_error, x0=init_guess, method='BFGS')
    a, b = result.x
    
    return a, b, result.fun


# â•â•â• ĞŸĞ Ğ˜ĞœĞ•Ğ : ĞŸĞ ĞĞ“ĞĞĞ—Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• (Ğ¤Ğ˜ĞĞĞ› 2024-25) â•â•â•
def forecast_example():
    """
    ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ Ğ½Ğ° 12 Ğ½ĞµĞ´ĞµĞ»ÑŒ
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (104 Ğ½ĞµĞ´ĞµĞ»Ğ¸)
    df = pd.read_csv(r"forecast1.csv")
    # df = df.dropna()
    df.loc[52, "ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸"] = (df.iloc[51]["ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸"]+df.iloc[53]["ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸"])/2
    t = np.arange(len(df))
    sales = df['ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸'].values
    
    # ĞŸĞ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
    params, error = fit_with_curve_fit(
        t, sales, forecast_model, 
        init_guess=[sales.mean(), 0, 0, 0]
    )
    
    print(f"ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: {params}")
    print(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {error:.2f}")
    
    # ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ½Ğ° 12 Ğ½ĞµĞ´ĞµĞ»ÑŒ
    future_t = np.arange(104, 104+12)
    forecast = forecast_model(future_t, *params)
    
    answer = ', '.join([f"{x:.2f}" for x in forecast])
    print(f"\nğŸ¯ ĞĞ¢Ğ’Ğ•Ğ¢: {answer}")
    
    true = np.array([653.07, 675.12, 694.82, 710.26, 720.55, 726.0, 728.04, 728.92, 731.16, 737.01, 747.9, 764.13])
    pred = np.array([float(f"{pred:.2f}") for pred in forecast])

    diff = true - pred
    norm = np.linalg.norm(diff)
    score = round(max(20 - norm / 6, 0))

    print("Ğ¡ĞºĞ¾Ñ€:", score)

    return answer, score

def grid_search(x, y, model_func, param_ranges):
    """
    param_ranges: ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ {'param_name': (min, max, steps)}
    ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: {'a': (1, 3, 100), 'b': (0, 1, 100)}
    """
    from itertools import product
    from tqdm import tqdm
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞµÑ‚ĞºÑƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
    grids = [np.linspace(min_val, max_val, steps) 
             for min_val, max_val, steps in param_ranges.values()]
    
    best_error = float('inf')
    best_params = None
    
    for params in tqdm(product(*grids), total=np.prod([steps for _, _, steps in param_ranges.values()])):
        predictions = model_func(x, *params)
        error = np.sum((y - predictions)**2)
        
        if error < best_error:
            best_error = error
            best_params = params
    
    return best_params, best_error

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. PANDAS Ğ¨ĞŸĞĞ Ğ“ĞĞ›ĞšĞ (Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def pandas_cheatsheet():
    """
    Ğ¢Ğ¸Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹
    """
    df = pd.read_csv('data.csv')
    
    # â•â•â• Ğ‘ĞĞ—ĞĞ’Ğ«Ğ• ĞĞŸĞ•Ğ ĞĞ¦Ğ˜Ğ˜ â•â•â•
    
    # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº
    answer = len(df)
    
    # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼
    answer = len(df[df['price'] < 1000])
    
    # Ğ”Ğ²Ğ° ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ (Ğ˜)
    answer = len(df[(df['area'] >= 70) & (df['metro_km'] < 1)])
    
    # Ğ”Ğ²Ğ° ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ (Ğ˜Ğ›Ğ˜)
    answer = len(df[(df['floor'] == 1) | (df['floor'] == 10)])
    
    # Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
    answer = df['price'].mean()
    
    # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾
    answer = len(df[df['price'] < df['price'].mean()])
    
    # ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼/Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼
    answer = df['price'].max()
    answer = df['price'].min()
    
    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€
    answer = df.sort_values('price').iloc[0]['price']  # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ½Ğ°
    
    # â•â•â• ĞŸĞ ĞĞŸĞ£Ğ©Ğ•ĞĞĞ«Ğ• Ğ—ĞĞĞ§Ğ•ĞĞ˜Ğ¯ â•â•â•
    
    # ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ğ¼Ğ¸
    cols_with_na = df.columns[df.isnull().any()].tolist()
    
    df[df.isnull().any(axis=1)]
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼
    df['column'].fillna(df['column'].mean(), inplace=True)
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ (Ğ’ĞĞ–ĞĞ Ğ”Ğ›Ğ¯ Ğ’Ğ Ğ•ĞœĞ•ĞĞĞ«Ğ¥ Ğ Ğ¯Ğ”ĞĞ’!)
    df['temperature'].interpolate(method='linear', inplace=True)
    
    # â•â•â• ĞšĞĞ Ğ Ğ•Ğ›Ğ¯Ğ¦Ğ˜Ğ¯ â•â•â•
    
    # ĞšĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹
    correlations = df.corr()['price'].abs()
    
    # Ğ¢Ñ€Ğ¸ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… (Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ°Ğ¼Ñƒ Ñ†ĞµĞ½Ñƒ)
    answer = correlations.nsmallest(4)[1:4].index.tolist()
    
    # Ğ¢Ñ€Ğ¸ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…
    answer = correlations.nlargest(4)[1:4].index.tolist()
    
    # â•â•â• Ğ“Ğ Ğ£ĞŸĞŸĞ˜Ğ ĞĞ’ĞšĞ â•â•â•
    
    # Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼
    answer = df.groupby('city')['price'].mean()
    
    # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼
    answer = df.groupby('category').size()
    
    # â•â•â• Ğ¡Ğ›ĞĞ–ĞĞ«Ğ• Ğ—ĞĞŸĞ ĞĞ¡Ğ« â•â•â•
    
    # Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ´ĞµÑˆĞµĞ²Ğ°Ñ ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ° Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸
    filtered = df[(df['area'] >= 90) & (df['kad_km'] < 1)]
    answer = filtered['price'].min()
    
    # Ğ Ğ°Ğ¹Ğ¾Ğ½ Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²
    district_counts = df['district'].value_counts()
    richest_district = district_counts.idxmax()
    answer = df[df['district'] == richest_district]['price'].min()
    
    # Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼
    answer = df[df['center_km'] < 3]['price_per_sqm'].mean()
    
    return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    
    # â•â•â• NLP: ĞšĞ›ĞĞ¡Ğ¡Ğ˜Ğ¤Ğ˜ĞšĞĞ¦Ğ˜Ğ¯ â•â•â•
    # answer = nlp_classification('texts.csv', 'text', 'label')
    
    # â•â•â• NLP: ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ â•â•â•
    # answer = nlp_clustering('texts.csv', 'text', n_clusters=2)
    
    # â•â•â• NLP: ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• Ğ¡Ğ›ĞĞ’Ğ â•â•â•
    # answer = nlp_keywords('docs.csv', 'text', doc_id=42, top_n=10)
    
    # â•â•â• Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯ â•â•â•
    # answer = logistic_regression_template('data.csv', 'target')
    
    # â•â•â• ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ĞœĞĞ”Ğ•Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• â•â•â•
    # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ñ‹
    # x = np.linspace(-2*np.pi, 2*np.pi, 100)
    # y = np.sin(2*x) + 0.7 + np.random.normal(0, 0.1, 100)
    # params, error = fit_with_minimize(x, y, sin_model, init_guess=[2, 0.7])
    # print(f"a={params[0]:.3f}, b={params[1]:.3f}, error={error:.3f}")
    # â•â•â• Ğ­Ğ›Ğ›Ğ˜ĞŸĞ¡ â•â•â•
    # df = pd.read_excel(r"DataModel1.xls", header=None)
    # df.columns = ["X", "Y"]
    # x_data = df["X"].to_numpy()
    # y_data = df["Y"].to_numpy()
    # a, b, error = fit_ellipse(x_data, y_data)
    # print(f"a={a:.3f}, b={b:.3f}")
    
    pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜ Ğ’ĞĞ–ĞĞ«Ğ• ĞœĞĞœĞ•ĞĞ¢Ğ«
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
1. NLTK Ğ”ĞĞĞĞ«Ğ•: Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ!
   nltk.download('stopwords')
   nltk.download('punkt')

2. ĞŸĞ ĞĞŸĞ£Ğ¡ĞšĞ˜ Ğ’ Ğ”ĞĞĞĞ«Ğ¥: 
   - Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ÑĞ´Ñ‹ â†’ interpolate(method='linear')
   - ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ â†’ fillna(mean)

3. ĞšĞĞ Ğ Ğ•Ğ›Ğ¯Ğ¦Ğ˜Ğ¯:
   - Ğ˜ÑĞºĞ°Ñ‚ÑŒ ĞœĞ˜ĞĞ˜ĞœĞĞ›Ğ¬ĞĞ£Ğ® Ğ¿Ğ¾ ĞœĞĞ”Ğ£Ğ›Ğ® Ğ´Ğ»Ñ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²

4. TF-IDF:
   - max_features=5000 - Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡
   - ngram_range=(1, 2) - ÑƒĞ½Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ + Ğ±Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹

5. Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯:
   - class_weight='balanced' - Ğ´Ğ»Ñ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
   - StandardScaler() - ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ!

6. ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ĞœĞĞ”Ğ•Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ•:
   - Ğ•ÑĞ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ĞĞ•Ğ¯Ğ’ĞĞĞ¯ â†’ minimize()
   - Ğ•ÑĞ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¯Ğ’ĞĞĞ¯ â†’ curve_fit()
   - ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾!

7. Ğ¤ĞĞ ĞœĞĞ¢ ĞĞ¢Ğ’Ğ•Ğ¢ĞĞ’:
   - Ğ¦ĞµĞ»Ğ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ â†’ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ¾
   - Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ‡Ğ¸ÑĞµĞ» â†’ "1.23, 4.56, 7.89"
   - Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ»Ğ¾Ğ² â†’ "ÑĞ»Ğ¾Ğ²Ğ¾1, ÑĞ»Ğ¾Ğ²Ğ¾2, ÑĞ»Ğ¾Ğ²Ğ¾3"
"""
