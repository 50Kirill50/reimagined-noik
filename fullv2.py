"""
Ğ ĞĞ—Ğ Ğ•Ğ¨Ğ•ĞĞĞ«Ğ• Ğ¡ĞĞ™Ğ¢Ğ« ĞĞ ĞĞ›Ğ˜ĞœĞŸĞ˜ĞĞ”Ğ•:
â€¢ https://docs.python.org
â€¢ https://scikit-learn.ru/
â€¢ https://pandas.pydata.org
â€¢ https://numpy.org/doc/
â€¢ https://www.nltk.org/
â€¢ https://www.geeksforgeeks.org
â€¢ https://education.yandex.ru/handbook
â€¢ https://jupyter.org/
â€¢ GitHub (Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ‘Ğ•Ğ— Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸)
â€¢ arXiv / Papers with Code / Hugging Face (Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹)
â€¢ Google Colab / Kaggle (Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ½Ğ¾ÑƒÑ‚Ğ±ÑƒĞºĞ¾Ğ² Ğ‘Ğ•Ğ— Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°)

Ğ¡ĞĞ”Ğ•Ğ Ğ–ĞĞĞ˜Ğ•:
1. ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ (NLTK, Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹)
2. ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ĞœĞĞ”Ğ•Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• (ĞšĞĞš ĞĞĞ™Ğ¢Ğ˜ INIT_GUESS!)
3. Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯ (Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ)
4. NLP + TF-IDF (5 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°)
5. PANDAS

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import pandas as pd
import numpy as np
import re
import string
from scipy.optimize import minimize, curve_fit
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, LinearRegression
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

# â•â•â• Ğ¡ĞšĞĞ§ĞĞ¢Ğ¬ NLTK Ğ”ĞĞĞĞ«Ğ• (ĞĞ”Ğ˜Ğ Ğ ĞĞ— ĞŸĞ•Ğ Ğ•Ğ” ĞĞ›Ğ˜ĞœĞŸĞ˜ĞĞ”ĞĞ™!) â•â•â•
# import nltk
# nltk.download('stopwords')
# nltk.download('punkt')


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ĞœĞĞ”Ğ•Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜ Ğ’ĞĞ–ĞĞ: ĞšĞĞš ĞĞĞ™Ğ¢Ğ˜ INIT_GUESS (Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞœĞ•Ğ¢ĞĞ” 1: Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ (Ğ“Ğ›ĞĞ’ĞĞ«Ğ™ Ğ¡ĞŸĞĞ¡ĞĞ‘!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
2. ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ñƒ ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹
3. ĞÑ†ĞµĞ½Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ "Ğ½Ğ° Ğ³Ğ»Ğ°Ğ·"

ĞœĞ•Ğ¢ĞĞ” 2: ĞĞĞĞ›Ğ˜Ğ— Ğ”ĞĞĞĞ«Ğ¥
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Ğ”Ğ»Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ y = a*x + b:
  a â‰ˆ (y.max() - y.min()) / (x.max() - x.min())  # ĞĞ°ĞºĞ»Ğ¾Ğ½
  b â‰ˆ y.mean() - a * x.mean()                     # Ğ¡Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ

- Ğ”Ğ»Ñ ÑĞ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ñ‹ y = A*sin(Ï‰*x) + B:
  A â‰ˆ (y.max() - y.min()) / 2      # ĞĞ¼Ğ¿Ğ»Ğ¸Ñ‚ÑƒĞ´Ğ°
  B â‰ˆ y.mean()                      # Ğ¡Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ
  Ï‰ â‰ˆ 2Ï€ / Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´                   # Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° (Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº!)

- Ğ”Ğ»Ñ ÑĞ»Ğ»Ğ¸Ğ¿ÑĞ° xÂ²/aÂ² + yÂ²/bÂ² = 1:
  a â‰ˆ x.abs().max() * 1.2
  b â‰ˆ y.abs().max() * 1.2

ĞœĞ•Ğ¢ĞĞ” 3: Ğ“Ğ Ğ£Ğ‘Ğ«Ğ™ ĞŸĞ•Ğ Ğ•Ğ‘ĞĞ  â†’ Ğ¢ĞĞ§ĞĞĞ¯ ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° grid search Ğ½Ğ° Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞµ
2. ĞŸĞ¾Ñ‚Ğ¾Ğ¼ minimize/curve_fit Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


# â•â•â• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ Ğ”Ğ›Ğ¯ Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ˜ Ğ”ĞĞĞĞ«Ğ¥ â•â•â•
def visualize_data(x, y, title="Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"):
    """
    ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - ĞŸĞ•Ğ Ğ’Ğ«Ğ™ Ğ¨ĞĞ“!
    """
    plt.figure(figsize=(10, 6))
    plt.scatter(x, y, alpha=0.5, s=20, label='Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.show()
    
    # ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ init_guess
    print("â•â•â• ĞŸĞĞ”Ğ¡ĞšĞĞ—ĞšĞ˜ Ğ”Ğ›Ğ¯ INIT_GUESS â•â•â•")
    print(f"x: min={x.min():.2f}, max={x.max():.2f}, mean={x.mean():.2f}")
    print(f"y: min={y.min():.2f}, max={y.max():.2f}, mean={y.mean():.2f}")
    print(f"Ğ Ğ°Ğ·Ğ¼Ğ°Ñ… y: {y.max() - y.min():.2f}")
    print(f"Ğ Ğ°Ğ·Ğ¼Ğ°Ñ… x: {x.max() - x.min():.2f}")


# â•â•â• Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ¡ ĞŸĞĞ”ĞĞ‘Ğ ĞĞĞĞĞ™ ĞœĞĞ”Ğ•Ğ›Ğ¬Ğ® â•â•â•
def visualize_fit(x, y, model_func, params, title="ĞŸĞ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"):
    """
    ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    """
    plt.figure(figsize=(10, 6))
    plt.scatter(x, y, alpha=0.5, s=20, label='Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ')
    
    # ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    x_smooth = np.linspace(x.min(), x.max(), 300)
    y_model = model_func(x_smooth, *params)
    plt.plot(x_smooth, y_model, 'r-', linewidth=2, label='ĞœĞ¾Ğ´ĞµĞ»ÑŒ')
    
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()


# â•â•â• ĞœĞ•Ğ¢ĞĞ” 1: MINIMIZE (Ğ£ĞĞ˜Ğ’Ğ•Ğ Ğ¡ĞĞ›Ğ¬ĞĞ«Ğ™) â•â•â•
def fit_with_minimize(x, y, model_func, init_guess):
    """
    ĞŸĞ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
    """
    def error_func(params):
        predictions = model_func(x, *params)
        return np.sum((y - predictions)**2)
    
    result = minimize(error_func, x0=init_guess, method='BFGS')
    
    return result.x, result.fun


# â•â•â• ĞœĞ•Ğ¢ĞĞ” 2: CURVE_FIT (Ğ”Ğ›Ğ¯ Ğ¯Ğ’ĞĞ«Ğ¥ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ™) â•â•â•
def fit_with_curve_fit(x, y, model_func, init_guess):
    """
    Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ ÑĞ²Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ y = f(x)
    """
    params, _ = curve_fit(model_func, x, y, p0=init_guess, maxfev=10000)
    
    predictions = model_func(x, *params)
    error = np.sum((y - predictions)**2)
    
    return params, error


# â•â•â• ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ â•â•â•

def linear_model(x, a, b):
    """y = a*x + b"""
    return a*x + b

def sin_model(x, a, b):
    """y = sin(a*x) + b"""
    return np.sin(a*x) + b

def forecast_model(t, a1, a2, a3, a4):
    """
    Ğ¤Ğ˜ĞĞĞ› 2024-25: ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶
    model(t) = a1 + a2*t + a3*tÂ² + a4*sinÂ²(2Ï€*t/13)
    """
    return a1 + a2*t + a3*t**2 + a4*np.sin(2*np.pi*t/13)**2

# df = pd.read_csv(r"forecast1.csv", parse_dates=['Ğ”Ğ°Ñ‚Ğ°'])
# df.loc[52, "ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸"] = (df.iloc[51]["ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸"]+df.iloc[53]["ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸"])/2
# df.columns = ["X", "Y"]
# t = np.arange(len(df))
# params = [16.70, 0.96, 0.0496, 23.08]
# visualize_fit(t, df["X"], forecast_model, params)

def ellipse_model(x, y):
    """
    Ğ¤Ğ˜ĞĞĞ› 2023-24: Ğ­Ğ»Ğ»Ğ¸Ğ¿Ñ (ĞĞ•Ğ¯Ğ’ĞĞĞ• Ğ£Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ•!)
    xÂ²/aÂ² + yÂ²/bÂ² = 1
    
    ĞšĞĞš ĞĞĞ™Ğ¢Ğ˜ INIT_GUESS:
    1. ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ scatter plot
    2. a â‰ˆ max(|x|) * 1.2
    3. b â‰ˆ max(|y|) * 1.2
    """
    def error_func(params):
        a, b = params
        residuals = (x**2 / a**2) + (y**2 / b**2) - 1
        return np.sum(residuals**2)
    
    # Init guess
    init_guess = [np.abs(x).max() * 1.2, np.abs(y).max() * 1.2]
    
    result = minimize(error_func, x0=init_guess, method='BFGS')
    a, b = result.x
    
    return a, b, result.fun


# â•â•â• ĞŸĞ Ğ˜ĞœĞ•Ğ : ĞŸĞĞ¨ĞĞ“ĞĞ’ĞĞ• Ğ Ğ•Ğ¨Ğ•ĞĞ˜Ğ• â•â•â•
def modeling_step_by_step():
    """
    ĞŸĞĞ›ĞĞ«Ğ™ ĞŸĞ Ğ˜ĞœĞ•Ğ : ĞºĞ°Ğº Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    df = pd.read_csv('data.csv')
    x = df['x'].values
    y = df['y'].values
    
    # Ğ¨ĞĞ“ 1: Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ (Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾!)
    visualize_data(x, y, "Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
    
    # Ğ¨ĞĞ“ 2: Ğ’Ğ«Ğ‘Ğ ĞĞ¢Ğ¬ ĞœĞĞ”Ğ•Ğ›Ğ¬ (Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºÑƒ)
    # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğµ Ğ½Ğ° ÑĞ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ñƒ â†’ sin_model
    # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğµ Ğ½Ğ° Ğ¿Ñ€ÑĞ¼ÑƒÑ â†’ linear_model
    # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğµ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ±Ğ¾Ğ»Ñƒ + ÑĞ¸Ğ½ÑƒÑ â†’ forecast_model
    
    # Ğ¨ĞĞ“ 3: ĞĞ¦Ğ•ĞĞ˜Ğ¢Ğ¬ INIT_GUESS
    # Ğ”Ğ»Ñ ÑĞ¸Ğ½ÑƒÑĞ¾Ğ¸Ğ´Ñ‹:
    amplitude = (y.max() - y.min()) / 2
    offset = y.mean()
    # Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° - Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğµ
    # Ğ•ÑĞ»Ğ¸ 2 Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ° Ğ½Ğ° [0, 2Ï€] â†’ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° â‰ˆ 2
    frequency = 2.0  # ĞÑ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºÑƒ!
    
    init_guess = [frequency, offset]
    
    # Ğ¨ĞĞ“ 4: ĞŸĞĞ”Ğ‘ĞĞ  ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ ĞĞ’
    params, error = fit_with_curve_fit(x, y, sin_model, init_guess)
    
    print(f"ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: a={params[0]:.3f}, b={params[1]:.3f}")
    print(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {error:.3f}")
    
    # Ğ¨ĞĞ“ 5: Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ
    visualize_fit(x, y, sin_model, params, "ĞŸĞ¾Ğ´Ğ¾Ğ³Ğ½Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
    
    return params


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯ (Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞĞ¯)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ĞœĞ•Ğ¢ĞĞ”Ğ« ĞšĞ›ĞĞ¡Ğ¡Ğ˜Ğ¤Ğ˜ĞšĞĞ¦Ğ˜Ğ˜ (ĞºĞ¾Ğ³Ğ´Ğ° Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞœĞ•Ğ¢ĞĞ” 1: Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ
âœ… Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ TF-IDF Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸
âœ… ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
âŒ Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°

ĞœĞ•Ğ¢ĞĞ” 2: NAIVE BAYES (Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… ĞĞ§Ğ•ĞĞ¬ Ğ‘Ğ«Ğ¡Ğ¢Ğ Ğ«Ğ™
âœ… Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸
âœ… ĞĞµ Ğ±Ğ¾Ğ¸Ñ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹
âŒ ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²

ĞœĞ•Ğ¢ĞĞ” 3: LINEAR SVM (ĞµÑĞ»Ğ¸ LogReg Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑÑ‹
âœ… Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ…
âŒ ĞœĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ LogReg

ĞœĞ•Ğ¢ĞĞ” 4: LOGISTIC REGRESSION CV (Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
âœ… ĞĞ°Ğ´ĞµĞ¶Ğ½ĞµĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ LogReg
âŒ Ğ§ÑƒÑ‚ÑŒ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

def classification_comparison(X_train, X_test, y_train, y_test):
    """
    Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
    """
    models = {
        'LogReg': LogisticRegression(max_iter=1000, class_weight='balanced'),
        'LogRegCV': LogisticRegressionCV(cv=5, max_iter=1000, class_weight='balanced'),
        'NaiveBayes': MultinomialNB(),
        'LinearSVM': LinearSVC(max_iter=1000, class_weight='balanced')
    }
    
    results = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        results[name] = accuracy
        print(f"{name}: {accuracy:.3f}")
    
    # Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹
    best_model_name = max(results, key=results.get)
    best_model = models[best_model_name]
    
    print(f"\nĞ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {best_model_name}")
    return best_model


def logistic_regression_full(csv_file, target_column):
    """
    ĞŸĞĞ›ĞĞ«Ğ™ Ğ¨ĞĞ‘Ğ›ĞĞ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
    """
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°
    df = pd.read_csv(csv_file)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ°
    print("Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¾Ğ²:")
    print(df[target_column].value_counts(normalize=True))
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ
    X = df.drop(target_column, axis=1)
    y = df[target_column]
    
    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    categorical_cols = X.select_dtypes(include=['object']).columns
    if len(categorical_cols) > 0:
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    
    # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²!)
    model = LogisticRegressionCV(
        cv=5,
        max_iter=10000,
        class_weight='balanced',
        random_state=42,
        scoring='accuracy'
    )
    model.fit(X_train_scaled, y_train)
    
    print(f"Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ C: {model.C_}")
    
    # ĞÑ†ĞµĞ½ĞºĞ°
    predictions = model.predict(X_test_scaled)
    print(f"\nĞ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: {accuracy_score(y_test, predictions):.3f}")
    print(classification_report(y_test, predictions))
    
    # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
    cm = confusion_matrix(y_test, predictions)
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, cmap='Blues')
    plt.colorbar()
    plt.xlabel('ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¾')
    plt.ylabel('Ğ˜ÑÑ‚Ğ¸Ğ½Ğ°')
    plt.title('ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº')
    for i in range(2):
        for j in range(2):
            plt.text(j, i, cm[i, j], ha='center', va='center')
    plt.show()
    
    # Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
    if hasattr(model, 'coef_'):
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': np.abs(model.coef_[0])
        }).sort_values('importance', ascending=False)
        
        print("\nĞ¢Ğ¾Ğ¿-10 Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²:")
        print(feature_importance.head(10))
        
        # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        plt.figure(figsize=(10, 6))
        plt.barh(feature_importance.head(10)['feature'], 
                 feature_importance.head(10)['importance'])
        plt.xlabel('Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ')
        plt.title('Ğ¢Ğ¾Ğ¿-10 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²')
        plt.tight_layout()
        plt.show()
    
    # ĞĞ¢Ğ’Ğ•Ğ¢ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    all_data_scaled = scaler.transform(X)
    all_predictions = model.predict(all_data_scaled)
    answer = (all_predictions == 1).sum()
    
    print(f"\nğŸ¯ ĞĞ¢Ğ’Ğ•Ğ¢: {answer}")
    return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. NLP + TF-IDF
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â• ĞŸĞ Ğ•Ğ”ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ â•â•â•
def preprocess_text(text, language='russian', 
                    remove_numbers=True, 
                    remove_stopwords=True, 
                    stem=True):
    """ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ°"""
    stop_words = set(stopwords.words(language))
    stemmer = SnowballStemmer(language)
    
    text = text.lower()
    
    if remove_numbers:
        text = re.sub(r'\d+', '', text)
    
    text = ''.join([c for c in text if c not in string.punctuation])
    words = text.split()
    
    if remove_stopwords:
        words = [w for w in words if w not in stop_words]
    
    if stem:
        words = [stemmer.stem(w) for w in words]
    
    return " ".join(words)


"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5 ĞœĞ•Ğ¢ĞĞ”ĞĞ’ Ğ Ğ•Ğ¨Ğ•ĞĞ˜Ğ¯ NLP Ğ—ĞĞ”ĞĞ§
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞœĞ•Ğ¢ĞĞ” 1: TF-IDF + Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯ (ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹
âœ… Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²

ĞœĞ•Ğ¢ĞĞ” 2: TF-IDF + ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ (ĞµÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ½ĞµÑ‚)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Ğ”Ğ»Ñ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
âŒ ĞÑƒĞ¶Ğ½Ğ¾ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ñ‡Ñ‚Ğ¾

ĞœĞ•Ğ¢ĞĞ” 3: ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ ĞĞ ĞĞ¡ĞĞĞ’Ğ• ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ¥ Ğ¡Ğ›ĞĞ’ (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… ĞÑ‡ĞµĞ½ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹
âœ… Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹
âŒ ĞÑƒĞ¶Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°

ĞœĞ•Ğ¢ĞĞ” 4: COUNT VECTORIZER + NAIVE BAYES (Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ TF-IDF
âœ… Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²

ĞœĞ•Ğ¢ĞĞ” 5: Ğ“Ğ˜Ğ‘Ğ Ğ˜Ğ”ĞĞ«Ğ™ (Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° + ML)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼
âœ… ĞŸĞ¾Ñ‚Ğ¾Ğ¼ ML Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# â•â•â• ĞœĞ•Ğ¢ĞĞ” 3: ĞšĞ›ĞĞ¡Ğ¡Ğ˜Ğ¤Ğ˜ĞšĞĞ¦Ğ˜Ğ¯ ĞŸĞ ĞŸĞ ĞĞ’Ğ˜Ğ›ĞĞœ â•â•â•
def classify_by_rules(df, text_column):
    """
    ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
    
    ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ vs Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸
    """
    # ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
    df['clean'] = df[text_column].apply(preprocess_text)
    
    # ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²
    review_keywords = ['Ğ¾Ñ‚ĞµĞ»', 'Ğ³Ğ¾ÑÑ‚Ğ¸Ğ½Ğ¸Ñ†', 'Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€', 'Ğ¾Ñ‚Ğ´Ñ‹Ñ…', 
                       'Ğ½Ğ¾Ğ¼ĞµÑ€', 'Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»', 'Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°Ğº', 'ÑĞµÑ€Ğ²Ğ¸Ñ']
    
    # ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹
    news_keywords = ['Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚', 'Ğ¿Ñ€ĞµÑÑÑĞ»ÑƒĞ¶Ğ±', 'Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚', 'Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²']
    
    # ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
    def classify(text):
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²
        has_review_words = any(word in text for word in review_keywords)
        has_news_words = any(word in text for word in news_keywords)
        
        if has_review_words and not has_news_words:
            return 1  # ĞÑ‚Ğ·Ñ‹Ğ²
        elif has_news_words and not has_review_words:
            return 0  # ĞĞ¾Ğ²Ğ¾ÑÑ‚ÑŒ
        else:
            return -1  # ĞĞµĞ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾
    
    df['prediction'] = df['clean'].apply(classify)
    
    # ĞĞ›Ğ¬Ğ¢Ğ•Ğ ĞĞĞ¢Ğ˜Ğ’ĞĞ«Ğ™ Ğ¡ĞŸĞĞ¡ĞĞ‘ (Ñ‡ĞµÑ€ĞµĞ· pandas):
    mask_review = (
        (df['clean'].str.contains('Ğ¾Ñ‚ĞµĞ»')) | 
        (df['clean'].str.contains('Ğ³Ğ¾ÑÑ‚Ğ¸Ğ½Ğ¸Ñ†')) |
        (df['clean'].str.contains('Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€')) |
        (df['clean'].str.contains('Ğ¾Ñ‚Ğ´Ñ‹Ñ…'))
    ) & (
        ~df['clean'].str.contains('Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚') &
        ~df['clean'].str.contains('Ğ¿Ñ€ĞµÑÑÑĞ»ÑƒĞ¶Ğ±')
    )
    
    df['prediction_alt'] = 0
    df.loc[mask_review, 'prediction_alt'] = 1
    
    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    print("Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼:")
    print(df['prediction'].value_counts())
    
    # ĞĞ¢Ğ’Ğ•Ğ¢
    answer = (df['prediction'] == 1).sum()
    print(f"\nğŸ¯ ĞĞ¢Ğ’Ğ•Ğ¢: {answer}")
    
    return answer


# â•â•â• Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ”Ğ›Ğ¯ NLP â•â•â•
def visualize_text_clusters(X, labels, n_samples=1000):
    """
    Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² (PCA)
    """
    from sklearn.decomposition import PCA
    
    # Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 2D
    pca = PCA(n_components=2, random_state=42)
    X_2d = pca.fit_transform(X[:n_samples].toarray())
    
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], 
                         c=labels[:n_samples], 
                         cmap='viridis', 
                         alpha=0.6, 
                         s=50)
    plt.colorbar(scatter)
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title('Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²')
    plt.grid(True, alpha=0.3)
    plt.show()


def visualize_word_importance(vectorizer, model, top_n=20):
    """
    Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
    """
    feature_names = vectorizer.get_feature_names_out()
    coefficients = model.coef_[0]
    
    # Ğ¢Ğ¾Ğ¿ ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ° 1
    top_positive = pd.DataFrame({
        'word': feature_names,
        'coef': coefficients
    }).sort_values('coef', ascending=False).head(top_n)
    
    # Ğ¢Ğ¾Ğ¿ ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ° 0
    top_negative = pd.DataFrame({
        'word': feature_names,
        'coef': coefficients
    }).sort_values('coef', ascending=True).head(top_n)
    
    # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    axes[0].barh(top_positive['word'], top_positive['coef'])
    axes[0].set_xlabel('ĞšĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚')
    axes[0].set_title('Ğ¢Ğ¾Ğ¿-20 ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ° 1')
    
    axes[1].barh(top_negative['word'], -top_negative['coef'])
    axes[1].set_xlabel('|ĞšĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚|')
    axes[1].set_title('Ğ¢Ğ¾Ğ¿-20 ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ° 0')
    
    plt.tight_layout()
    plt.show()


# â•â•â• ĞŸĞĞ›ĞĞ«Ğ™ ĞŸĞ Ğ˜ĞœĞ•Ğ  NLP â•â•â•
def nlp_full_pipeline(csv_file, text_column, label_column=None):
    """
    ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ NLP Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹
    """
    df = pd.read_csv(csv_file)
    
    # ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
    print("ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²...")
    df['clean'] = df[text_column].apply(preprocess_text)
    
    # Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    print("Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ...")
    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X = vectorizer.fit_transform(df['clean'])
    
    if label_column:
        # Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 1: ĞšĞ›ĞĞ¡Ğ¡Ğ˜Ğ¤Ğ˜ĞšĞĞ¦Ğ˜Ğ¯
        y = df[label_column]
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )
        
        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        model = LogisticRegression(max_iter=1000, class_weight='balanced')
        model.fit(X_train, y_train)
        
        # ĞÑ†ĞµĞ½ĞºĞ°
        predictions = model.predict(X_test)
        print(f"\nĞ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: {accuracy_score(y_test, predictions):.3f}")
        
        # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
        visualize_word_importance(vectorizer, model)
        
        # ĞĞ¢Ğ’Ğ•Ğ¢
        all_predictions = model.predict(X)
        answer = (all_predictions == 1).sum()
        
    else:
        # Ğ¡Ğ¦Ğ•ĞĞĞ Ğ˜Ğ™ 2: ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ²
        feature_names = vectorizer.get_feature_names_out()
        for i in range(2):
            print(f"\n{'='*50}")
            print(f"ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ  {i} (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€: {(labels == i).sum()})")
            
            cluster_center = kmeans.cluster_centers_[i]
            top_indices = cluster_center.argsort()[-15:][::-1]
            top_words = [feature_names[idx] for idx in top_indices]
            print(f"ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°: {', '.join(top_words)}")
        
        # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        visualize_text_clusters(X, labels)
        
        # ĞĞ¢Ğ’Ğ•Ğ¢ (Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ!)
        review_cluster = 0  # Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ¬!
        answer = (labels == review_cluster).sum()
    
    print(f"\nğŸ¯ ĞĞ¢Ğ’Ğ•Ğ¢: {answer}")
    return answer


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. PANDAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def pandas_cheatsheet():
    """Ğ¢Ğ¸Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹"""
    df = pd.read_csv('data.csv')
    
    # â•â•â• Ğ‘ĞĞ—ĞĞ’Ğ«Ğ• ĞĞŸĞ•Ğ ĞĞ¦Ğ˜Ğ˜ â•â•â•
    answer = len(df)
    answer = len(df[df['price'] < 1000])
    answer = len(df[(df['area'] >= 70) & (df['metro_km'] < 1)])
    answer = len(df[(df['floor'] == 1) | (df['floor'] == 10)])
    
    # Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ
    answer = df['price'].mean()
    answer = len(df[df['price'] < df['price'].mean()])
    
    # ĞœĞ¸Ğ½/Ğ¼Ğ°ĞºÑ
    answer = df['price'].max()
    answer = df.sort_values('price').iloc[0]['price']
    
    # â•â•â• ĞŸĞ ĞĞŸĞ£Ğ©Ğ•ĞĞĞ«Ğ• Ğ—ĞĞĞ§Ğ•ĞĞ˜Ğ¯ â•â•â•
    cols_with_na = df.columns[df.isnull().any()].tolist()
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ
    df['column'].fillna(df['column'].mean(), inplace=True)
    
    # Ğ’ĞĞ–ĞĞ: Ğ”Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²!
    df['temperature'].interpolate(method='linear', inplace=True)
    
    # â•â•â• ĞšĞĞ Ğ Ğ•Ğ›Ğ¯Ğ¦Ğ˜Ğ¯ â•â•â•
    correlations = df.corr()['price'].abs()
    answer = correlations.nsmallest(4)[1:4].index.tolist()
    answer = correlations.nlargest(4)[1:4].index.tolist()

    # â•â•â• Ğ“Ğ Ğ£ĞŸĞŸĞ˜Ğ ĞĞ’ĞšĞ â•â•â•
    answer = df.groupby('city')['price'].mean()
    answer = df.groupby('category').size()

    # â•â•â• Ğ¡Ğ›ĞĞ–ĞĞ«Ğ• Ğ—ĞĞŸĞ ĞĞ¡Ğ« â•â•â•
    filtered = df[(df['area'] >= 90) & (df['kad_km'] < 1)]
    answer = filtered['price'].min()

    # Ğ Ğ°Ğ¹Ğ¾Ğ½ Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞºĞ¾Ğ»
    richest_district = df['district'].value_counts().idxmax()
    answer = df[df['district'] == richest_district]['price'].min()

    # Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼
    answer = df[df['center_km'] < 3]['price_per_sqm'].mean()

    return answer

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. ĞŸĞĞ›Ğ•Ğ—ĞĞ«Ğ• Ğ¡Ğ¡Ğ«Ğ›ĞšĞ˜ Ğ”Ğ›Ğ¯ ĞŸĞĞ˜Ğ¡ĞšĞ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ˜ ĞĞ ĞĞ›Ğ˜ĞœĞŸĞ˜ĞĞ”Ğ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Ğ§Ğ¢Ğ Ğ˜Ğ¡ĞšĞĞ¢Ğ¬ ĞĞ Ğ ĞĞ—Ğ Ğ•Ğ¨Ğ•ĞĞĞ«Ğ¥ Ğ¡ĞĞ™Ğ¢ĞĞ¥:

PANDAS (https://pandas.pydata.org)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ĞŸĞ¾Ğ¸ÑĞº: "pandas filter by condition"
â€¢ ĞŸĞ¾Ğ¸ÑĞº: "pandas groupby"
â€¢ ĞŸĞ¾Ğ¸ÑĞº: "pandas fillna"
â€¢ ĞŸĞ¾Ğ¸ÑĞº: "pandas interpolate"
SCIKIT-LEARN (https://scikit-learn.ru/)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ LogisticRegression - Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹
â€¢ TfidfVectorizer - Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
â€¢ KMeans - ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
SCIPY (https://docs.scipy.org)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ scipy.optimize.minimize - Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹
â€¢ scipy.optimize.curve_fit
NUMPY (https://numpy.org/doc/)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (sin, cos, exp)
â€¢ ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ°Ğ¼Ğ¸
GEEKSFORGEEKS (https://www.geeksforgeeks.org)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ "logistic regression python example"
â€¢ "tfidf sklearn example"
â€¢ "scipy curve fit example"
GITHUB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ğ˜ÑĞºĞ°Ñ‚ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ:
â€¢ "sklearn tfidf classification github"
â€¢ "scipy minimize ellipse fitting github"
ARXIV (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ´Ğ»Ñ VPD (vapor pressure deficit)
â€¢ Ğ£Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ½ Ğ´ĞµÑ€ Ğ’Ğ°Ğ°Ğ»ÑŒÑĞ°
â€¢ ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ¯ ĞŸĞĞ˜Ğ¡ĞšĞ:

Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ñ‰Ğ¸ Ğ² pandas/sklearn Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸
Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ½Ğ°ÑˆĞµĞ» â†’ GeeksForGeeks
Ğ•ÑĞ»Ğ¸ ÑĞ¾Ğ²ÑĞµĞ¼ Ğ½Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ â†’ GitHub Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹
Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ¸Ğ· ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ â†’ arXiv
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "main":
    # # â•â•â• ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ĞœĞĞ”Ğ•Ğ›Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• â•â•â•
    # modeling_step_by_step()

    # # â•â•â• Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯ â•â•â•
    # answer = logistic_regression_full('data.csv', 'target')

    # # â•â•â• NLP: ĞŸĞĞ›ĞĞ«Ğ™ ĞŸĞĞ™ĞŸĞ›ĞĞ™Ğ â•â•â•
    # answer = nlp_full_pipeline('texts.csv', 'text', 'label')

    # # â•â•â• NLP: ĞŸĞ ĞŸĞ ĞĞ’Ğ˜Ğ›ĞĞœ â•â•â•
    # answer = classify_by_rules(df, 'text')

    pass

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜ Ğ’ĞĞ–ĞĞ«Ğ• ĞœĞĞœĞ•ĞĞ¢Ğ«
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Ğ’Ğ˜Ğ—Ğ£ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ - Ğ’Ğ¡Ğ•Ğ“Ğ”Ğ ĞŸĞ•Ğ Ğ’Ğ«Ğ™ Ğ¨ĞĞ“!
â€¢ ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â€¢ ĞÑ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ "Ğ½Ğ° Ğ³Ğ»Ğ°Ğ·"
â€¢ ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
INIT_GUESS:
â€¢ Ğ”Ğ»Ñ ÑĞ¸Ğ½ÑƒÑĞ°: Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸ Ğ½Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´, Ğ°Ğ¼Ğ¿Ğ»Ğ¸Ñ‚ÑƒĞ´Ñƒ
â€¢ Ğ”Ğ»Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹: Ğ½Ğ°ĞºĞ»Ğ¾Ğ½ â‰ˆ (y_max - y_min) / (x_max - x_min)
â€¢ Ğ”Ğ»Ñ ÑĞ»Ğ»Ğ¸Ğ¿ÑĞ°: a, b â‰ˆ max|x|, max|y| * 1.2
ĞŸĞ ĞĞŸĞ£Ğ¡ĞšĞ˜ Ğ’ Ğ”ĞĞĞĞ«Ğ¥:
â€¢ Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ÑĞ´Ñ‹ â†’ df['col'].interpolate(method='linear')
â€¢ ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ â†’ df['col'].fillna(mean)
ĞšĞĞ Ğ Ğ•Ğ›Ğ¯Ğ¦Ğ˜Ğ¯:
â€¢ Ğ˜ÑĞºĞ°Ñ‚ÑŒ ĞœĞ˜ĞĞ˜ĞœĞĞ›Ğ¬ĞĞ£Ğ® Ğ¿Ğ¾ ĞœĞĞ”Ğ£Ğ›Ğ® Ğ´Ğ»Ñ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ…
NLP:
â€¢ Ğ•ÑĞ»Ğ¸ Ğ¼ĞµÑ‚ĞºĞ¸ ĞµÑÑ‚ÑŒ â†’ TF-IDF + LogReg
â€¢ Ğ•ÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ½ĞµÑ‚ â†’ ĞšĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
â€¢ Ğ•ÑĞ»Ğ¸ Ğ·Ğ½Ğ°ĞµÑˆÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° â†’ ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°
Ğ›ĞĞ“Ğ˜Ğ¡Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ“Ğ Ğ•Ğ¡Ğ¡Ğ˜Ğ¯:
â€¢ class_weight='balanced' - Ğ´Ğ»Ñ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…
â€¢ StandardScaler() - ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ!
â€¢ LogisticRegressionCV - Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
Ğ ĞĞ—Ğ Ğ•Ğ¨Ğ•ĞĞĞ«Ğ• Ğ¡ĞĞ™Ğ¢Ğ«:
â€¢ pandas.pydata.org - Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
â€¢ scikit-learn.ru - Ğ´Ğ»Ñ ML
â€¢ geeksforgeeks.org - Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ¾Ğ´Ğ°
â€¢ GitHub - Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ
"""
